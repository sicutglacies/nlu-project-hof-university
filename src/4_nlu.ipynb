{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick = np.load('../data/additional_info_dict.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Scim: Intelligent Faceted Highlights for Interactive, Multi-Pass Skimming of Scientific Papers',\n",
       " 'abstract': 'Researchers are expected to keep up with an immense literature, yet often find it prohibitively time-consuming to do so. This paper ex-plores how intelligent agents can help scaffold in-situ information seeking across scientific papers. Specifically, we present Scim, an AI-augmented reading interface designed to help researchers skim papers by automatically identifying, classifying, and highlighting salient sentences, organized into rhetorical facets rooted in common information needs. Using Scim as a design probe, we explore the benefits and drawbacks of imperfect AI assistance within an augmented reading interface. We found researchers used Scim in several different ways: from reading primarily in the ‘highlight browser’ (side panel) to making multiple passes through the paper with different facets activated (e.g., focusing solely on objective and novelty in their first pass). From our study, we identify six key design recommendations and avenues for future research in augmented reading interfaces.',\n",
       " 'year': 2022,\n",
       " 'fieldsOfStudy': ['Computer Science'],\n",
       " 'authors': [{'authorId': '27083453', 'name': 'Raymond Fok'},\n",
       "  {'authorId': '2065039588', 'name': 'Andrew Head'},\n",
       "  {'authorId': '2699105', 'name': 'Jonathan Bragg'},\n",
       "  {'authorId': '46258841', 'name': 'Kyle Lo'},\n",
       "  {'authorId': '1716902', 'name': 'Marti A. Hearst'},\n",
       "  {'authorId': '1780531', 'name': 'Daniel S. Weld'}],\n",
       " 'tldr': {'model': 'tldr@v2.0.0',\n",
       "  'text': 'Scim is presented, an AI-augmented reading interface designed to help researchers skim papers by automatically identifying, classifying, and highlighting salient sentences, organized into rhetorical facets rooted in common information needs.'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick.item().get('d1ca07561b24afe8b1bd18dd1c239dbbbd221964')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(quick.item().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for key in keys:\n",
    "    d = quick.item()[key]\n",
    "    text = ''\n",
    "    if d['title'] is not None:\n",
    "        text += d['title'] + '\\n'\n",
    "    if d['abstract'] is not None:\n",
    "        text += d['abstract'] + '\\n'\n",
    "    if d['tldr'] is not None:\n",
    "        text += d['tldr']['text']\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['text', 'video', 'audio', 'speech', 'image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\n",
    "    'English', 'Chinese', 'Spanish', 'Hindi', 'Bengali', 'Portuguese', 'Russian', \n",
    "    'Japanese', 'Vietnamese', 'German', 'French', 'Turkish', 'Korean', 'Italian',\n",
    "    'Polish', 'Dutch', 'Indonesian', 'Thai', 'Danish', 'Czech', 'Finnish', 'Greek',\n",
    "    'Swedish', 'Hungarian', 'Latvian', 'Lithuanian', 'Estonian', 'Arabic', 'Multilingual'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_functions = [\n",
    "    'relu', 'silu', 'gelu', 'sigmoid', 'tanh', 'elu', 'softmax',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = [\n",
    "    {'CNN': 'cnn'}, \n",
    "    {'DNN': ['dnn', 'ann']}, # should be treated together\n",
    "    {'RNN' : ['rnn']}, \n",
    "    {'LSTM' : ['lstm']}, \n",
    "    {'GRU' : ['gru']}, \n",
    "    {'GAN' : ['gan']}, \n",
    "    {'VAE' : ['vae']}, \n",
    "    {'seq2seq' : ['seq2seq']}, \n",
    "    {'BERT': ['bert']}, \n",
    "    {'Transformer' : ['transformer']},\n",
    "    {'GPT' : ['gpt']}, \n",
    "    {'GPT-2': ['gpt2', 'gpt-2']}, # should be treated together\n",
    "    {'GPT-3': ['gpt3', 'gpt-3']}, # should be treated together\n",
    "    {'AE': ['ae', 'autoencoder']}, # should be treated together\n",
    "    {'ResNet': ['resnet']},\n",
    "    {'attention': ['attention']},\n",
    "    {'NER': ['ner']},\n",
    "    {'ViT': ['vit']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24811/24811 [10:07<00:00, 40.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(keys):\n",
    "    d = quick.item()[key]\n",
    "    text = ''\n",
    "    if d['title'] is not None:\n",
    "        text += d['title'] + '\\n'\n",
    "    if d['abstract'] is not None:\n",
    "        text += d['abstract'] + '\\n'\n",
    "    if d['tldr'] is not None:\n",
    "        text += d['tldr']['text']\n",
    "        \n",
    "    quick.item()[key]['language'] = []\n",
    "    quick.item()[key]['act_function'] = []\n",
    "    quick.item()[key]['architecture'] = []\n",
    "    quick.item()[key]['data_type'] = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text in languages:\n",
    "            quick.item()[key]['language'].append(token.text)\n",
    "        if token.text.lower() in act_functions:\n",
    "            quick.item()[key]['act_function'].append(token.text)\n",
    "        for arch in architectures:\n",
    "            arch_key = list(arch.keys())[0]\n",
    "            if token.text.lower() in arch[arch_key]:\n",
    "                quick.item()[key]['architecture'].append(arch_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type_preds = classifier(texts, data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types_preds_words = list(map(lambda x: x['labels'][0], data_type_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24811it [00:00, 925679.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for key, dt in tqdm(zip(keys, data_types_preds_words)):\n",
    "    quick.item()[key]['data_type'] = [dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24811/24811 [00:00<00:00, 213189.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(keys):\n",
    "    for name in ['language', 'act_function', 'architecture']:\n",
    "        quick.item()[key][name] = list(set(quick.item()[key][name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/full_data_dict.npy', quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/lists/data_types.npy', np.array(data_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/lists/act_functions', np.array(act_functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/lists/languages.npy', np.array(languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [list(arch.keys())[0] for arch in architectures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Adversarial vulnerability of powerful near out-of-distribution detection', 'abstract': 'There has been a significant progress in detecting out-of-distribution (OOD) inputs in neural networks recently, primarily due to the use of large models pretrained on large datasets, and an emerging use of multi-modality. We show a severe adversarial vulnerability of even the strongest current OOD detection techniques. With a small, targeted perturbation to the input pixels, we can change the image assignment from an in-distribution to an out-distribution, and vice versa, easily. In particular, we demonstrate severe adversarial vulnerability on the challenging near OOD CIFAR-100 vs CIFAR-10 task, as well as on the far OOD CIFAR-100 vs SVHN. We study the adversarial robustness of several post-processing techniques, including the simple baseline of Maximum of Softmax Probabilities (MSP), the Mahalanobis distance, and the newly proposed Relative Mahalanobis distance. By comparing the loss of OOD detection performance at various perturbation strengths, we demonstrate the beneficial effect of using ensembles of OOD detectors, and the use of the Relative Mahalanobis distance over other postprocessing methods. In addition, we show that even strong zero-shot OOD detection using CLIP and multi-modality suffers from a severe lack of adversarial robustness as well. Our code is available on GitHub.', 'year': 2022, 'fieldsOfStudy': ['Computer Science'], 'authors': [{'authorId': '30176974', 'name': 'Stanislav Fort'}], 'tldr': {'model': 'tldr@v2.0.0', 'text': 'This work shows a severe adversarial vulnerability of even the strongest current OOD detection techniques, and studies the adversarial robustness of several post-processing techniques, including the simple baseline of Maximum of Softmax Probabilities (MSP), the Mahalanobis distance, and the newly proposed Relative MahalanOBis distance.'}, 'language': [], 'act_function': ['Softmax'], 'architecture': [], 'data_type': ['image'], 'authors_string': 'Stanislav Fort'}\n",
      "\n",
      "{'title': 'SGD Noise and Implicit Low-Rank Bias in Deep Neural Networks', 'abstract': 'We analyze deep ReLU neural networks trained with mini-batch stochastic gradient decent and weight decay. We prove that the source of the SGD noise is an implicit low rank constraint across all of the weight matrices within the network. Furthermore, we show, both theoretically and empirically, that when training a neural network using Stochastic Gradient Descent (SGD) with a small batch size, the resulting weight matrices are expected to be of small rank. Our analysis relies on a minimal set of assumptions and the neural networks may include convolutional layers, residual connections, as well as batch normalization layers. Abstract We analyze deep ReLU neural networks trained with mini-batch Stochastic Gradient Descent (SGD) and weight decay. We study the source of SGD noise and prove that the only convergence points of mini-batch SGD are zero functions. Furthermore, we show, both theoretically and empirically, that when training a neural network using SGD with a small batch size, the resulting weight matrices are expected to be of small rank. Our analysis relies on a minimal set of assumptions and the neural networks may include residual connections, as well as batch normalization layers.', 'year': 2022, 'fieldsOfStudy': ['Computer Science', 'Mathematics'], 'authors': [{'authorId': '9923405', 'name': 'Tomer Galanti'}, {'authorId': '1685292', 'name': 'T. Poggio'}], 'tldr': {'model': 'tldr@v2.0.0', 'text': 'It is shown, both theoretically and empirically, that when training a neural network using Stochastic Gradient Descent (SGD) with a small batch size, the resulting weight matrices are expected to be of small rank.'}, 'language': [], 'act_function': ['ReLU'], 'architecture': [], 'data_type': ['text'], 'authors_string': 'Tomer Galanti, T. Poggio'}\n",
      "\n",
      "{'title': 'Research on Dual Channel News Headline Classification Based on ERNIE Pre-training Model', 'abstract': 'The classification of news headlines is an important direction in the field of NLP, and its data has the characteristics of compactness, uniqueness and various forms. Aiming at the problem that the traditional neural network model cannot adequately capture the underlying feature information of the data and cannot jointly extract key global features and deep local features, a dual-channel network model DC-EBAD based on the ERNIE pre-training model is proposed. Use ERNIE to extract the lexical, semantic and contextual feature information at the bottom of the text, generate dynamic word vector representations fused with context, and then use the BiLSTM-AT network channel to secondary extract the global features of the data and use the attention mechanism to give key parts higher The weight of the DPCNN channel is used to overcome the long-distance text dependence problem and obtain deep local features. The local and global feature vectors are spliced, and finally passed to the fully connected layer, and the final classification result is output through Softmax. The experimental results show that the proposed model improves the accuracy, precision and F1-score of news headline classification compared with the traditional neural network model and the single-channel model under the same conditions. It can be seen that it can perform well in the multi-classification application of news headline text under large data volume.', 'year': 2022, 'fieldsOfStudy': ['Computer Science'], 'authors': [{'authorId': '2259852', 'name': 'Jianing Li'}, {'authorId': '2044804764', 'name': 'Hui Cao'}], 'tldr': {'model': 'tldr@v2.0.0', 'text': 'The proposed dual-channel network model DC-EBAD based on the ERNIE pre-training model improves the accuracy, precision and F1-score of news headline classification compared with the traditional neural network model and the single-channel model under the same conditions.'}, 'language': [], 'act_function': ['Softmax'], 'architecture': ['attention'], 'data_type': ['text'], 'authors_string': 'Jianing Li, Hui Cao'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for key in keys:\n",
    "    if len(quick.item()[key]['act_function']) != 0:\n",
    "        print(quick.item()[key])\n",
    "        print()\n",
    "        n += 1\n",
    "        if n == 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    if quick.item()[key]['authors'] is not None:\n",
    "        quick.item()[key]['authors_string'] = ', '.join(list(map(lambda x: x['name'], quick.item()[key]['authors'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Scim: Intelligent Faceted Highlights for Interactive, Multi-Pass Skimming of Scientific Papers',\n",
       " 'abstract': 'Researchers are expected to keep up with an immense literature, yet often find it prohibitively time-consuming to do so. This paper ex-plores how intelligent agents can help scaffold in-situ information seeking across scientific papers. Specifically, we present Scim, an AI-augmented reading interface designed to help researchers skim papers by automatically identifying, classifying, and highlighting salient sentences, organized into rhetorical facets rooted in common information needs. Using Scim as a design probe, we explore the benefits and drawbacks of imperfect AI assistance within an augmented reading interface. We found researchers used Scim in several different ways: from reading primarily in the ‘highlight browser’ (side panel) to making multiple passes through the paper with different facets activated (e.g., focusing solely on objective and novelty in their first pass). From our study, we identify six key design recommendations and avenues for future research in augmented reading interfaces.',\n",
       " 'year': 2022,\n",
       " 'fieldsOfStudy': ['Computer Science'],\n",
       " 'authors': [{'authorId': '27083453', 'name': 'Raymond Fok'},\n",
       "  {'authorId': '2065039588', 'name': 'Andrew Head'},\n",
       "  {'authorId': '2699105', 'name': 'Jonathan Bragg'},\n",
       "  {'authorId': '46258841', 'name': 'Kyle Lo'},\n",
       "  {'authorId': '1716902', 'name': 'Marti A. Hearst'},\n",
       "  {'authorId': '1780531', 'name': 'Daniel S. Weld'}],\n",
       " 'tldr': {'model': 'tldr@v2.0.0',\n",
       "  'text': 'Scim is presented, an AI-augmented reading interface designed to help researchers skim papers by automatically identifying, classifying, and highlighting salient sentences, organized into rhetorical facets rooted in common information needs.'},\n",
       " 'language': [],\n",
       " 'act_function': [],\n",
       " 'architecture': [],\n",
       " 'data_type': ['text'],\n",
       " 'authors_string': 'Raymond Fok, Andrew Head, Jonathan Bragg, Kyle Lo, Marti A. Hearst, Daniel S. Weld'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick.item()[keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    res = quick.item()[key]['year']\n",
    "    if res is not None:\n",
    "        years.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/lists/years.npy', np.array(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1967, 1990, 2015, 2016, 2018, 2019, 2020, 2021, 2022}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    'Art',\n",
    "    'Biology',\n",
    "    'Business',\n",
    "    'Chemistry',\n",
    "    'Computer Science',\n",
    "    'Economics',\n",
    "    'Engineering',\n",
    "    'Environmental Science',\n",
    "    'Geography',\n",
    "    'Geology',\n",
    "    'History',\n",
    "    'Materials Science',\n",
    "    'Mathematics',\n",
    "    'Medicine',\n",
    "    'Philosophy',\n",
    "    'Physics',\n",
    "    'Political Science',\n",
    "    'Psychology',\n",
    "    'Sociology'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/lists/topics.npy', np.array(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cit_rel = np.load('../data/citation_relations.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = {}\n",
    "for elem in cit_rel:\n",
    "    key = list(elem.keys())[0]\n",
    "    ddd[key] = elem[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/cit_rel_dict.npy', np.array(ddd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq = np.load('../data/lists/unique_ids_list.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relu', 'silu', 'gelu', 'sigmoid', 'tanh', 'elu', 'softmax']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddt = np.load('../data/full_data_dict.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data.unique_ids_list.npy', np.array(list(ddt.item().keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
